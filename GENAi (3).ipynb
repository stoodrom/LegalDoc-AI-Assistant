{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip show openai\n",
        "!pip install --upgrade openai\n",
        "!pip install retry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkxy2pi4dWVM",
        "outputId": "84cfeebb-0097-41f2-a342-eb135ed8a173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: openai\n",
            "Version: 1.76.2\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: https://github.com/openai/openai-python\n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: \n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.76.2)\n",
            "Collecting openai\n",
            "  Downloading openai-1.77.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Downloading openai-1.77.0-py3-none-any.whl (662 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m662.0/662.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.76.2\n",
            "    Uninstalling openai-1.76.2:\n",
            "      Successfully uninstalled openai-1.76.2\n",
            "Successfully installed openai-1.77.0\n",
            "Collecting retry\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.11/dist-packages (from retry) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry)\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: py, retry\n",
            "Successfully installed py-1.11.0 retry-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5mMfiF_f9Hc",
        "outputId": "4ca26d7b-d498-4d9f-f677-03a6a68d6de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.37.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.45.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.45.0 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En1DAFuNhWkn",
        "outputId": "6cd7ae95-28fe-4929-dddc-faa22e6f3f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.7-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.7-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No need to run"
      ],
      "metadata": {
        "id": "CdrFgTRqB2iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit openai retry pyngrok langchain langchain-openai langchain-community faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQkCBPAAdP5q",
        "outputId": "f50bf35b-47c4-4b66-ee9d-4880d53e05f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.77.0)\n",
            "Requirement already satisfied: retry in /usr/local/lib/python3.11/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.7)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.11/dist-packages (from retry) (4.4.2)\n",
            "Requirement already satisfied: py<2.0.0,>=1.4.26 in /usr/local/lib/python3.11/dist-packages (from retry) (1.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Collecting langchain-core<1.0.0,>=0.3.55 (from langchain)\n",
            "  Downloading langchain_core-0.3.59-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.37.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-0.3.16-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.59-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, tiktoken, pydantic-settings, dataclasses-json, langchain-core, langchain-openai, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.56\n",
            "    Uninstalling langchain-core-0.3.56:\n",
            "      Successfully uninstalled langchain-core-0.3.56\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.11.0 httpx-sse-0.4.0 langchain-community-0.3.23 langchain-core-0.3.59 langchain-openai-0.3.16 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip\n",
        "from google.colab import userdata\n",
        "!./ngrok authtoken 2WZ7G7R4czwBsrarBNoY4f06ceA_2wBhmXvF12TFAvqYbTKb9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZtLyTgSdXRw",
        "outputId": "bd207f83-3204-437b-b573-4656ea3368f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-07 20:13:46--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 75.2.60.68, 35.71.179.82, 99.83.220.108, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|75.2.60.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‚Äòngrok-stable-linux-amd64.zip‚Äô\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  5.31MB/s    in 2.5s    \n",
            "\n",
            "2025-05-07 20:13:50 (5.31 MB/s) - ‚Äòngrok-stable-linux-amd64.zip‚Äô saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import openai\n",
        "import os\n",
        "import json\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "0fMkrBCmYTif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "openai.api_key = \"OPENAI_API_KEY\"\n",
        "GOOGLE_API_KEY = \"your-google-api-key-here\"\n",
        "GOOGLE_CSE_ID = \"your-google-cse-id-here\"\n",
        "OUTPUT_DIR = Path(\"generated_docs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "EMBEDDINGS_MODEL = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai.api_key)\n",
        "\n",
        "# === SESSION SETUP ===\n",
        "if 'case_type' not in st.session_state:\n",
        "    st.session_state.case_type = None\n",
        "if 'role' not in st.session_state:\n",
        "    st.session_state.role = None\n",
        "if 'case_id' not in st.session_state:\n",
        "    st.session_state.case_id = str(uuid.uuid4())\n",
        "if 'vector_store' not in st.session_state:\n",
        "    st.session_state.vector_store = None\n",
        "\n",
        "st.title(\"üìö Legal Assistant Platform\")\n",
        "\n",
        "# === CASE TYPE SELECTION ===\n",
        "case_type = st.selectbox(\"Step 1: Select Case Type\", [\"\", \"Civil\", \"Criminal\"])\n",
        "st.session_state.case_type = case_type\n",
        "\n",
        "# === LAWYER ROLE SELECTION ===\n",
        "role = None\n",
        "if case_type == \"Civil\":\n",
        "    role = st.selectbox(\"Step 2: Select Lawyer Role (Civil)\", [\"\", \"Plaintiff Lawyer\", \"Defendant Lawyer\"])\n",
        "elif case_type == \"Criminal\":\n",
        "    role = st.selectbox(\"Step 2: Select Lawyer Role (Criminal)\", [\"\", \"Prosecution\", \"Defendant\"])\n",
        "st.session_state.role = role\n",
        "\n",
        "# === UTILITIES ===\n",
        "def save_document(content: str, filename: str) -> str:\n",
        "    filepath = OUTPUT_DIR / f\"{st.session_state.case_id}_{filename}\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(content)\n",
        "    return str(filepath)\n",
        "\n",
        "def generate_openai_doc(prompt: str, temperature: float = 0.6, max_tokens: int = 800) -> str:\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\", #changed to gpt-4 as gpt-4o is not a valid model\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal advisor for U.S. attorneys. Be detailed, accurate, and cite relevant statutes.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        st.error(f\"OpenAI API Error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def create_vector_store(convo, notes, uploaded_texts):\n",
        "    documents = []\n",
        "    if convo:\n",
        "        documents.append(Document(page_content=convo, metadata={\"source\": \"conversation\"}))\n",
        "    if notes:\n",
        "        documents.append(Document(page_content=notes, metadata={\"source\": \"notes\"}))\n",
        "    for i, txt in enumerate(uploaded_texts):\n",
        "        documents.append(Document(page_content=txt, metadata={\"source\": f\"upload_{i}\"}))\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    splits = splitter.split_documents(documents)\n",
        "    return FAISS.from_documents(splits, EMBEDDINGS_MODEL)\n",
        "\n",
        "def extract_text_from_file(upload):\n",
        "    try:\n",
        "        return upload.read().decode(\"utf-8\")\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def run_argument_suggestion(vector_store):\n",
        "    try:\n",
        "        retriever_docs = vector_store.similarity_search(\"legal case context\", k=5)\n",
        "        combined = \"\\n\".join(doc.page_content for doc in retriever_docs)\n",
        "        prompt = f\"\"\"\n",
        "        You are a legal strategist. Based on the following documents and context, suggest three strong legal arguments with citations to relevant U.S. statutes or .gov legal resources. If applicable, mention the act or law.\n",
        "        Documents:\n",
        "        {combined}\n",
        "        \"\"\"\n",
        "        result = generate_openai_doc(prompt, temperature=0.7, max_tokens=600)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"Error generating arguments: {str(e)}\"\n",
        "\n",
        "def google_legal_search(query):\n",
        "    try:\n",
        "        q = f\"{query} site:uscourts.gov OR site:law.cornell.edu OR site:uniformlaws.org\"\n",
        "        url = f\"https://www.googleapis.com/customsearch/v1?q={q}&key={GOOGLE_API_KEY}&cx={GOOGLE_CSE_ID}\"\n",
        "        response = requests.get(url).json()\n",
        "        items = response.get(\"items\", [])\n",
        "        if items:\n",
        "            top = items[0]\n",
        "            title = top.get(\"title\")\n",
        "            link = top.get(\"link\")\n",
        "            snippet = top.get(\"snippet\")\n",
        "            return f\"{snippet}\\nSource: {link}\"\n",
        "        else:\n",
        "            return \"No legal reference found in .gov search.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error during Google search: {e}\"\n",
        "\n",
        "def run_qa(question, vector_store):\n",
        "    try:\n",
        "        docs = vector_store.similarity_search(question, k=3)\n",
        "        context = \"\\n\".join(doc.page_content for doc in docs)\n",
        "        prompt = f\"\"\"\n",
        "        Use the following legal case context to answer the question.\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        If the answer is not clearly found in context, search trusted .gov websites and provide reference.\n",
        "        \"\"\"\n",
        "        answer = generate_openai_doc(prompt)\n",
        "        if \"not found\" in answer.lower():\n",
        "            search_result = google_legal_search(question)\n",
        "            answer += f\"\\n\\nExternal Reference:\\n{search_result}\"\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"Error during Q&A: {e}\"\n",
        "\n",
        "# === ROLE WORKFLOW ===\n",
        "if case_type and role:\n",
        "    st.markdown(f\"### You selected: {case_type} ‚Üí {role}\")\n",
        "\n",
        "    # Common Inputs\n",
        "    st.subheader(\"üß† Conversation\")\n",
        "    conversation = st.text_area(\"Enter the conversation text or dialogue\")\n",
        "\n",
        "    st.subheader(\"üìÅ Upload Documents\")\n",
        "    uploaded_files = st.file_uploader(\"Upload any related files (summons, evidence, etc.)\", accept_multiple_files=True)\n",
        "    uploaded_texts = [extract_text_from_file(f) for f in uploaded_files if f is not None]\n",
        "\n",
        "    st.subheader(\"üóíÔ∏è Sidebar Notes\")\n",
        "    notes = st.sidebar.text_area(\"Sidebar Notes (Optional)\", placeholder=\"Type notes here...\")\n",
        "\n",
        "    # Build Vector DB\n",
        "    if st.button(\"üíæ Build Case Vector Store\"):\n",
        "        st.session_state.vector_store = create_vector_store(conversation, notes, uploaded_texts)\n",
        "        st.success(\"Vector store created for case context.\")\n",
        "\n",
        "    # Argument Suggestion\n",
        "    st.subheader(\"üìã Suggest Arguments\")\n",
        "    if st.button(\"Suggest Arguments\"):\n",
        "        if st.session_state.vector_store:\n",
        "            arguments = run_argument_suggestion(st.session_state.vector_store)\n",
        "            st.text_area(\"Suggested Arguments\", arguments, height=300)\n",
        "        else:\n",
        "            st.warning(\"Please build the vector store first.\")\n",
        "\n",
        "    # Q&A\n",
        "    st.subheader(\"‚ùì Q&A\")\n",
        "    question = st.text_input(\"Ask a legal question\")\n",
        "# === CONFIGURATION ===\n",
        "openai.api_key = \"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\"\n",
        "GOOGLE_API_KEY = \"your-google-api-key-here\"\n",
        "GOOGLE_CSE_ID = \"your-google-cse-id-here\"\n",
        "OUTPUT_DIR = Path(\"generated_docs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "EMBEDDINGS_MODEL = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai.api_key)\n",
        "\n",
        "# === SESSION SETUP ===\n",
        "if 'case_type' not in st.session_state:\n",
        "    st.session_state.case_type = None\n",
        "if 'role' not in st.session_state:\n",
        "    st.session_state.role = None\n",
        "if 'case_id' not in st.session_state:\n",
        "    st.session_state.case_id = str(uuid.uuid4())\n",
        "if 'vector_store' not in st.session_state:\n",
        "    st.session_state.vector_store = None\n",
        "\n",
        "st.title(\"üìö Legal Assistant Platform\")\n",
        "\n",
        "# === CASE TYPE SELECTION ===\n",
        "case_type = st.selectbox(\"Step 1: Select Case Type\", [\"\", \"Civil\", \"Criminal\"])\n",
        "st.session_state.case_type = case_type\n",
        "\n",
        "# === LAWYER ROLE SELECTION ===\n",
        "role = None\n",
        "if case_type == \"Civil\":\n",
        "    role = st.selectbox(\"Step 2: Select Lawyer Role (Civil)\", [\"\", \"Plaintiff Lawyer\", \"Defendant Lawyer\"])\n",
        "elif case_type == \"Criminal\":\n",
        "    role = st.selectbox(\"Step 2: Select Lawyer Role (Criminal)\", [\"\", \"Prosecution\", \"Defendant\"])\n",
        "st.session_state.role = role\n",
        "\n",
        "# === UTILITIES ===\n",
        "def save_document(content: str, filename: str) -> str:\n",
        "    filepath = OUTPUT_DIR / f\"{st.session_state.case_id}_{filename}\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(content)\n",
        "    return str(filepath)\n",
        "\n",
        "def generate_openai_doc(prompt: str, temperature: float = 0.6, max_tokens: int = 800) -> str:\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\", #changed to gpt-4 as gpt-4o is not a valid model\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal advisor for U.S. attorneys. Be detailed, accurate, and cite relevant statutes.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        st.error(f\"OpenAI API Error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def create_vector_store(convo, notes, uploaded_texts):\n",
        "    documents = []\n",
        "    if convo:\n",
        "        documents.append(Document(page_content=convo, metadata={\"source\": \"conversation\"}))\n",
        "    if notes:\n",
        "        documents.append(Document(page_content=notes, metadata={\"source\": \"notes\"}))\n",
        "    for i, txt in enumerate(uploaded_texts):\n",
        "        documents.append(Document(page_content=txt, metadata={\"source\": f\"upload_{i}\"}))\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    splits = splitter.split_documents(documents)\n",
        "    return FAISS.from_documents(splits, EMBEDDINGS_MODEL)\n",
        "\n",
        "def extract_text_from_file(upload):\n",
        "    try:\n",
        "        return upload.read().decode(\"utf-8\")\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def run_argument_suggestion(vector_store):\n",
        "    try:\n",
        "        retriever_docs = vector_store.similarity_search(\"legal case context\", k=5)\n",
        "        combined = \"\\n\".join(doc.page_content for doc in retriever_docs)\n",
        "        prompt = f\"\"\"\n",
        "        You are a legal strategist. Based on the following documents and context, suggest three strong legal arguments with citations to relevant U.S. statutes or .gov legal resources. If applicable, mention the act or law.\n",
        "        Documents:\n",
        "        {combined}\n",
        "        \"\"\"\n",
        "        result = generate_openai_doc(prompt, temperature=0.7, max_tokens=600)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"Error generating arguments: {str(e)}\"\n",
        "\n",
        "def google_legal_search(query):\n",
        "    try:\n",
        "        q = f\"{query} site:uscourts.gov OR site:law.cornell.edu OR site:uniformlaws.org\"\n",
        "        url = f\"https://www.googleapis.com/customsearch/v1?q={q}&key={GOOGLE_API_KEY}&cx={GOOGLE_CSE_ID}\"\n",
        "        response = requests.get(url).json()\n",
        "        items = response.get(\"items\", [])\n",
        "        if items:\n",
        "            top = items[0]\n",
        "            title = top.get(\"title\")\n",
        "            link = top.get(\"link\")\n",
        "            snippet = top.get(\"snippet\")\n",
        "            return f\"{snippet}\\nSource: {link}\"\n",
        "        else:\n",
        "            return \"No legal reference found in .gov search.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error during Google search: {e}\"\n",
        "\n",
        "def run_qa(question, vector_store):\n",
        "    try:\n",
        "        docs = vector_store.similarity_search(question, k=3)\n",
        "        context = \"\\n\".join(doc.page_content for doc in docs)\n",
        "        prompt = f\"\"\"\n",
        "        Use the following legal case context to answer the question.\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        If the answer is not clearly found in context, search trusted .gov websites and provide reference.\n",
        "        \"\"\"\n",
        "        answer = generate_openai_doc(prompt)\n",
        "        if \"not found\" in answer.lower():\n",
        "            search_result = google_legal_search(question)\n",
        "            answer += f\"\\n\\nExternal Reference:\\n{search_result}\"\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"Error during Q&A: {e}\"\n",
        "\n",
        "# === ROLE WORKFLOW ===\n",
        "if case_type and role:\n",
        "    st.markdown(f\"### You selected: {case_type} ‚Üí {role}\")\n",
        "\n",
        "    # Common Inputs\n",
        "    st.subheader(\"üß† Conversation\")\n",
        "    conversation = st.text_area(\"Enter the conversation text or dialogue\")\n",
        "\n",
        "    st.subheader(\"üìÅ Upload Documents\")\n",
        "    uploaded_files = st.file_uploader(\"Upload any related files (summons, evidence, etc.)\", accept_multiple_files=True)\n",
        "    uploaded_texts = [extract_text_from_file(f) for f in uploaded_files if f is not None]\n",
        "\n",
        "    st.subheader(\"üóíÔ∏è Sidebar Notes\")\n",
        "    notes = st.sidebar.text_area(\"Sidebar Notes (Optional)\", placeholder=\"Type notes here...\")\n",
        "\n",
        "    # Build Vector DB\n",
        "    if st.button(\"üíæ Build Case Vector Store\"):\n",
        "        st.session_state.vector_store = create_vector_store(conversation, notes, uploaded_texts)\n",
        "        st.success(\"Vector store created for case context.\")\n",
        "\n",
        "    # Argument Suggestion\n",
        "    st.subheader(\"üìã Suggest Arguments\")\n",
        "    if st.button(\"Suggest Arguments\"):\n",
        "        if st.session_state.vector_store:\n",
        "            arguments = run_argument_suggestion(st.session_state.vector_store)\n",
        "            st.text_area(\"Suggested Arguments\", arguments, height=300)\n",
        "        else:\n",
        "            st.warning(\"Please build the vector store first.\")\n",
        "\n",
        "    # Q&A\n",
        "    st.subheader(\"‚ùì Q&A\")\n",
        "    question = st.text_input(\"Ask a legal question\")\n",
        "    if st.button(\"Get Answer\"):\n",
        "        if st.session_state.vector_store:\n",
        "            answer = run_qa(question, st.session_state.vector_store)\n",
        "            st.text_area(\"Answer\", answer, height=300)\n",
        "        else:\n",
        "            st.warning(\"Please build the vector store first.\")\n",
        "import json\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "openai.api_key = \"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\"\n",
        "GOOGLE_API_KEY = \"your-google-api-key-here\"\n",
        "GOOGLE_CSE_ID = \"your-google-cse-id-here\"\n",
        "OUTPUT_DIR = Path(\"generated_docs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "EMBEDDINGS_MODEL = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai.api_key)\n",
        "\n",
        "# === SESSION SETUP ===\n",
        "if 'case_type' not in st.session_state:\n",
        "    st.session_state.case_type = None\n",
        "if 'role' not in st.session_state:\n",
        "    st.session_state.role = None\n",
        "if 'case_id' not in st.session_state:\n",
        "    st.session_state.case_id = str(uuid.uuid4())\n",
        "if 'vector_store' not in st.session_state:\n",
        "    st.session_state.vector_store = None\n",
        "\n",
        "st.title(\"üìö Legal Assistant Platform\")\n",
        "\n",
        "# === CASE TYPE SELECTION ===\n",
        "case_type = st.selectbox(\"Step 1: Select Case Type\", [\"\", \"Civil\", \"Criminal\"])\n",
        "st.session_state.case_type = case_type\n",
        "\n",
        "# === LAWYER ROLE SELECTION ===\n",
        "role = None\n",
        "if case_type == \"Civil\":\n",
        "    role = st.selectbox(\"Step 2: Select Lawyer Role (Civil)\", [\"\", \"Plaintiff Lawyer\", \"Defendant Lawyer\"])\n",
        "elif case_type == \"Criminal\":\n",
        "    role = st.selectbox(\"Step 2: Select Lawyer Role (Criminal)\", [\"\", \"Prosecution\", \"Defendant\"])\n",
        "st.session_state.role = role\n",
        "\n",
        "# === UTILITIES ===\n",
        "def save_document(content: str, filename: str) -> str:\n",
        "    filepath = OUTPUT_DIR / f\"{st.session_state.case_id}_{filename}\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(content)\n",
        "    return str(filepath)\n",
        "\n",
        "def generate_openai_doc(prompt: str, temperature: float = 0.6, max_tokens: int = 800) -> str:\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\", #changed to gpt-4 as gpt-4o is not a valid model\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal advisor for U.S. attorneys. Be detailed, accurate, and cite relevant statutes.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        st.error(f\"OpenAI API Error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def create_vector_store(convo, notes, uploaded_texts):\n",
        "    documents = []\n",
        "    if convo:\n",
        "        documents.append(Document(page_content=convo, metadata={\"source\": \"conversation\"}))\n",
        "    if notes:\n",
        "        documents.append(Document(page_content=notes, metadata={\"source\": \"notes\"}))\n",
        "    for i, txt in enumerate(uploaded_texts):\n",
        "        documents.append(Document(page_content=txt, metadata={\"source\": f\"upload_{i}\"}))\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    splits = splitter.split_documents(documents)\n",
        "    return FAISS.from_documents(splits, EMBEDDINGS_MODEL)\n",
        "\n",
        "def extract_text_from_file(upload):\n",
        "    try:\n",
        "        return upload.read().decode(\"utf-8\")\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def run_argument_suggestion(vector_store):\n",
        "    try:\n",
        "        retriever_docs = vector_store.similarity_search(\"legal case context\", k=5)\n",
        "        combined = \"\\n\".join(doc.page_content for doc in retriever_docs)\n",
        "        prompt = f\"\"\"\n",
        "        You are a legal strategist. Based on the following documents and context, suggest three strong legal arguments with citations to relevant U.S. statutes or .gov legal resources. If applicable, mention the act or law.\n",
        "        Documents:\n",
        "        {combined}\n",
        "        \"\"\"\n",
        "        result = generate_openai_doc(prompt, temperature=0.7, max_tokens=600)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"Error generating arguments: {str(e)}\"\n",
        "\n",
        "def google_legal_search(query):\n",
        "    try:\n",
        "        q = f\"{query} site:uscourts.gov OR site:law.cornell.edu OR site:uniformlaws.org\"\n",
        "        url = f\"https://www.googleapis.com/customsearch/v1?q={q}&key={GOOGLE_API_KEY}&cx={GOOGLE_CSE_ID}\"\n",
        "        response = requests.get(url).json()\n",
        "        items = response.get(\"items\", [])\n",
        "        if items:\n",
        "            top = items[0]\n",
        "            title = top.get(\"title\")\n",
        "            link = top.get(\"link\")\n",
        "            snippet = top.get(\"snippet\")\n",
        "            return f\"{snippet}\\nSource: {link}\"\n",
        "        else:\n",
        "            return \"No legal reference found in .gov search.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error during Google search: {e}\"\n",
        "\n",
        "def run_qa(question, vector_store):\n",
        "    try:\n",
        "        docs = vector_store.similarity_search(question, k=3)\n",
        "        context = \"\\n\".join(doc.page_content for doc in docs)\n",
        "        prompt = f\"\"\"\n",
        "        Use the following legal case context to answer the question.\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        If the answer is not clearly found in context, search trusted .gov websites and provide reference.\n",
        "        \"\"\"\n",
        "        answer = generate_openai_doc(prompt)\n",
        "        if \"not found\" in answer.lower():\n",
        "            search_result = google_legal_search(question)\n",
        "            answer += f\"\\n\\nExternal Reference:\\n{search_result}\"\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"Error during Q&A: {e}\"\n",
        "\n",
        "# === ROLE WORKFLOW ===\n",
        "if case_type and role:\n",
        "    st.markdown(f\"### You selected: {case_type} ‚Üí {role}\")\n",
        "\n",
        "    # Common Inputs\n",
        "    st.subheader(\"üß† Conversation\")\n",
        "    conversation = st.text_area(\"Enter the conversation text or dialogue\")\n",
        "\n",
        "    st.subheader(\"üìÅ Upload Documents\")\n",
        "    uploaded_files = st.file_uploader(\"Upload any related files (summons, evidence, etc.)\", accept_multiple_files=True)\n",
        "    uploaded_texts = [extract_text_from_file(f) for f in uploaded_files if f is not None]\n",
        "\n",
        "    st.subheader(\"üóíÔ∏è Sidebar Notes\")\n",
        "    notes = st.sidebar.text_area(\"Sidebar Notes (Optional)\", placeholder=\"Type notes here...\")\n",
        "\n",
        "    # Build Vector DB\n",
        "    if st.button(\"üíæ Build Case Vector Store\"):\n",
        "        st.session_state.vector_store = create_vector_store(conversation, notes, uploaded_texts)\n",
        "        st.success(\"Vector store created for case context.\")\n",
        "\n",
        "    # Argument Suggestion\n",
        "    st.subheader(\"üìã Suggest Arguments\")\n",
        "    if st.button(\"Suggest Arguments\"):\n",
        "        if st.session_state.vector_store:\n",
        "            arguments = run_argument_suggestion(st.session_state.vector_store)\n",
        "            st.text_area(\"Suggested Arguments\", arguments, height=300)\n",
        "        else:\n",
        "            st.warning(\"Please build the vector store first.\")\n",
        "\n",
        "    # Q&A\n",
        "    st.subheader(\"‚ùì Q&A\")\n",
        "    question = st.text_input(\"Ask a legal question\")\n",
        "    if st.button(\"Get Answer\"):\n",
        "        if st.session_state.vector_store:\n",
        "            answer = run_qa(question, st.session_state.vector_store)\n",
        "            st.text_area(\"Answer\", answer)\n",
        "if 'case_type' not in st.session_state:\n",
        "    st.session_state.case_type = None\n",
        "if 'role' not in st.session_state:\n",
        "    st.session_state.role = None\n",
        "if 'case_id' not in st.session_state:\n",
        "    st.session_state.case_id = str(uuid.uuid4())\n",
        "if 'vector_store' not in st.session_state:\n",
        "    st.session_state.vector_store = None\n",
        "\n",
        "st.title(\"üìö Legal Assistant Platform\")\n",
        "\n",
        "# === CASE TYPE SELECTION ===\n",
        "case_type = st.selectbox(\"Step 1: Select Case Type\", [\"\", \"Civil\", \"Criminal\"])\n",
        "st.session_state.case_type = case_type\n",
        "\n",
        "# === LAWYER ROLE SELECTION ===\n",
        "role = None\n",
        "if case_type == \"Civil\":\n",
        "    role = st.selectbox(\"Step 2: Select Lawyer Role (Civil)\", [\"\", \"Plaintiff Lawyer\", \"Defendant Lawyer\"])\n",
        "elif case_type == \"Criminal\":\n",
        "    role = st.selectbox(\"Step 2: Select Lawyer Role (Criminal)\", [\"\", \"Prosecution\", \"Defendant\"])\n",
        "st.session_state.role = role\n",
        "\n",
        "# === UTILITIES ===\n",
        "def save_document(content: str, filename: str) -> str:\n",
        "    filepath = OUTPUT_DIR / f\"{st.session_state.case_id}_{filename}\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(content)\n",
        "    return str(filepath)\n",
        "\n",
        "def generate_openai_doc(prompt: str, temperature: float = 0.6, max_tokens: int = 800) -> str:\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal advisor for U.S. attorneys. Be detailed, accurate, and cite relevant statutes.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        st.error(f\"OpenAI API Error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def create_vector_store(convo, notes, uploaded_texts):\n",
        "    documents = []\n",
        "    if convo:\n",
        "        documents.append(Document(page_content=convo, metadata={\"source\": \"conversation\"}))\n",
        "    if notes:\n",
        "        documents.append(Document(page_content=notes, metadata={\"source\": \"notes\"}))\n",
        "    for i, txt in enumerate(uploaded_texts):\n",
        "        documents.append(Document(page_content=txt, metadata={\"source\": f\"upload_{i}\"}))\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    splits = splitter.split_documents(documents)\n",
        "    return FAISS.from_documents(splits, EMBEDDINGS_MODEL)\n",
        "\n",
        "def extract_text_from_file(upload):\n",
        "    try:\n",
        "        return upload.read().decode(\"utf-8\")\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def run_argument_suggestion(vector_store):\n",
        "    try:\n",
        "        retriever_docs = vector_store.similarity_search(\"legal case context\", k=5)\n",
        "        combined = \"\\n\".join(doc.page_content for doc in retriever_docs)\n",
        "        prompt = f\"\"\"\n",
        "        You are a legal strategist. Based on the following documents and context, suggest three strong legal arguments with citations to relevant U.S. statutes or .gov legal resources. If applicable, mention the act or law.\n",
        "        Documents:\n",
        "        {combined}\n",
        "        \"\"\"\n",
        "        result = generate_openai_doc(prompt, temperature=0.7, max_tokens=600)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"Error generating arguments: {str(e)}\"\n",
        "\n",
        "def google_legal_search(query):\n",
        "    try:\n",
        "        q = f\"{query} site:uscourts.gov OR site:law.cornell.edu OR site:uniformlaws.org\"\n",
        "        url = f\"https://www.googleapis.com/customsearch/v1?q={q}&key={GOOGLE_API_KEY}&cx={GOOGLE_CSE_ID}\"\n",
        "        response = requests.get(url).json()\n",
        "        items = response.get(\"items\", [])\n",
        "        if items:\n",
        "            top = items[0]\n",
        "            title = top.get(\"title\")\n",
        "            link = top.get(\"link\")\n",
        "            snippet = top.get(\"snippet\")\n",
        "            return f\"{snippet}\\nSource: {link}\"\n",
        "        else:\n",
        "            return \"No legal reference found in .gov search.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error during Google search: {e}\"\n",
        "\n",
        "def run_qa(question, vector_store):\n",
        "    try:\n",
        "        docs = vector_store.similarity_search(question, k=3)\n",
        "        context = \"\\n\".join(doc.page_content for doc in docs)\n",
        "        prompt = f\"\"\"\n",
        "        Use the following legal case context to answer the question.\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        If the answer is not clearly found in context, search trusted .gov websites and provide reference.\n",
        "        \"\"\"\n",
        "        answer = generate_openai_doc(prompt)\n",
        "        if \"not found\" in answer.lower():\n",
        "            search_result = google_legal_search(question)\n",
        "            answer += f\"\\n\\nExternal Reference:\\n{search_result}\"\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"Error during Q&A: {e}\"\n",
        "\n",
        "# === ROLE WORKFLOW ===\n",
        "if case_type and role:\n",
        "    st.markdown(f\"### You selected: {case_type} ‚Üí {role}\")\n",
        "\n",
        "    # Common Inputs\n",
        "    st.subheader(\"üß† Conversation\")\n",
        "    conversation = st.text_area(\"Enter the conversation text or dialogue\")\n",
        "\n",
        "    st.subheader(\"üìÅ Upload Documents\")\n",
        "    uploaded_files = st.file_uploader(\"Upload any related files (summons, evidence, etc.)\", accept_multiple_files=True)\n",
        "    uploaded_texts = [extract_text_from_file(f) for f in uploaded_files if f is not None]\n",
        "\n",
        "    st.subheader(\"üóíÔ∏è Sidebar Notes\")\n",
        "    notes = st.sidebar.text_area(\"Sidebar Notes (Optional)\", placeholder=\"Type notes here...\")\n",
        "\n",
        "    # Build Vector DB\n",
        "    if st.button(\"üíæ Build Case Vector Store\"):\n",
        "        st.session_state.vector_store = create_vector_store(conversation, notes, uploaded_texts)\n",
        "        st.success(\"Vector store created for case context.\")\n",
        "\n",
        "    # Argument Suggestion\n",
        "    st.subheader(\"üìã Suggest Arguments\")\n",
        "    if st.button(\"Suggest Arguments\"):\n",
        "        if st.session_state.vector_store:\n",
        "            arguments = run_argument_suggestion(st.session_state.vector_store)\n",
        "            st.text_area(\"Suggested Arguments\", arguments, height=300)\n",
        "        else:\n",
        "            st.warning(\"Please build the vector store first.\")\n",
        "\n",
        "    # Q&A\n",
        "    st.subheader(\"‚ùì Q&A\")\n",
        "    question = st.text_input(\"Ask a legal question\")\n",
        "    if st.button(\"Get Answer\"):\n",
        "        if st.session_state.vector_store:\n",
        "            answer = run_qa(question, st.session_state.vector_store)\n",
        "            st.text_area(\"Answer\", answer, height=300)\n",
        "        else:\n",
        "            st.warning(\"Please build the vector store first.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSdyBH4kdqO5",
        "outputId": "b2915796-ae38-4e3c-babe-ab57b2f75204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-07 20:28:19.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.216 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.221 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.222 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.223 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.224 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.225 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.226 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.227 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.228 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.229 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.230 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.231 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.232 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.233 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.234 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.419 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.420 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.421 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.422 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.423 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.424 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.425 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.426 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.428 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.432 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.433 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.433 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.620 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.623 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.624 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.625 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.626 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.626 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.627 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.628 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.628 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.630 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.631 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.632 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.633 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.634 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.635 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.638 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.639 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.640 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.641 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.642 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.643 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.643 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.644 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.645 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.646 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.648 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.650 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-07 20:28:19.651 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Set ngrok authtoken\n",
        "try:\n",
        "    ngrok_auth_token = userdata.get(\"NGRO_AUTH\")\n",
        "    if not ngrok_auth_token:\n",
        "        raise ValueError(\"NGROK_AUTH_TOKEN not found in Colab Secrets.\")\n",
        "    conf.get_default().auth_token = ngrok_auth_token\n",
        "    print(\"ngrok authtoken set successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting ngrok authtoken: {e}\")\n",
        "    raise\n",
        "\n",
        "# Kill existing ngrok processes\n",
        "!pkill ngrok\n",
        "ngrok.kill()\n",
        "print(\"Existing ngrok sessions terminated.\")\n",
        "\n",
        "# Start Streamlit in the background\n",
        "os.system(\"streamlit run app.py &\")\n",
        "\n",
        "# Create a tunnel on port 8501\n",
        "try:\n",
        "    public_url = ngrok.connect(8501, bind_tls=True)\n",
        "    print(\"Streamlit app is live at:\", public_url)\n",
        "except Exception as e:\n",
        "    print(f\"Error creating ngrok tunnel: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtarIX4ud0M_",
        "outputId": "8a1a58b8-d579-411a-e42f-6c7606c725c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok authtoken set successfully.\n",
            "Existing ngrok sessions terminated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-05-07T20:28:35+0000 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://68e3-35-229-186-32.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "print(\"API Key:\", api_key if api_key else \"Not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBaKumkGHaYJ",
        "outputId": "f1e11c29-f45b-4ab2-98ee-16a9997d0004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key: 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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "n4jGiM2zxPee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import openai\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "try:\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n",
        "        max_tokens=10\n",
        "    )\n",
        "    print(\"API key works! Response:\", response.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(\"API key error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owAq0T8RHwxR",
        "outputId": "7958adcf-0a92-4746-89df-84c69b4c4d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key works! Response: Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 -1"
      ],
      "metadata": {
        "id": "P9Iqopx9LJbq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}